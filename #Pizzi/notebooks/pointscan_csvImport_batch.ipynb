{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03153fea-4cc9-4ddf-9da9-e276ef0401cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tkinter import Tk, filedialog\n",
    "\n",
    "# Open dialog to select a folder\n",
    "root = Tk()\n",
    "root.withdraw()  # Hide the root window\n",
    "input_dir = filedialog.askdirectory(title=\"Select folder with output_roi CSVs\")\n",
    "\n",
    "# Collect only .csv files with 'output_roi' in the filename\n",
    "csv_files = [f for f in os.listdir(input_dir) if f.endswith(\".csv\") and \"output_roi\" in f]\n",
    "\n",
    "# Read each CSV into a dictionary\n",
    "csv_data = {\n",
    "    filename: pd.read_csv(os.path.join(input_dir, filename), header=None)\n",
    "    for filename in csv_files\n",
    "}\n",
    "\n",
    "# Report status and preview one file\n",
    "print(f\"{len(csv_data)} CSV files loaded from: {input_dir}\")\n",
    "if csv_data:\n",
    "    example_key = next(iter(csv_data))\n",
    "    print(f\"Example file: {example_key}\")\n",
    "    display(csv_data[example_key].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df521d34-e127-4943-ac33-2b4e7c1ad1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(csv_data.keys())\n",
    "csv_data['output_roi2_frame1_Z-38.7.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef95777c-b090-4f7d-a2c0-031ebdd35897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: /Users/jcoleman/Documents/--LARGE DATA--/#Pizzi/TBI-sham rbc velocity/results/070825_pointscan_csvFiles/mht01_roiA/mht01_roiA_roi_signals.pkl\n",
      "‚úÖ Saved: /Users/jcoleman/Documents/--LARGE DATA--/#Pizzi/TBI-sham rbc velocity/results/070825_pointscan_csvFiles/mht03_roiA/mht03_roiA_roi_signals.pkl\n",
      "‚úÖ Saved: /Users/jcoleman/Documents/--LARGE DATA--/#Pizzi/TBI-sham rbc velocity/results/070825_pointscan_csvFiles/mht03_roiB/mht03_roiB_roi_signals.pkl\n",
      "‚úÖ Saved: /Users/jcoleman/Documents/--LARGE DATA--/#Pizzi/TBI-sham rbc velocity/results/070825_pointscan_csvFiles/mht05_roiA/mht05_roiA_roi_signals.pkl\n",
      "‚úÖ Saved: /Users/jcoleman/Documents/--LARGE DATA--/#Pizzi/TBI-sham rbc velocity/results/070825_pointscan_csvFiles/mht05_roiB/mht05_roiB_roi_signals.pkl\n",
      "‚úÖ Saved: /Users/jcoleman/Documents/--LARGE DATA--/#Pizzi/TBI-sham rbc velocity/results/070825_pointscan_csvFiles/mht07_roiA/mht07_roiA_roi_signals.pkl\n",
      "‚úÖ Saved: /Users/jcoleman/Documents/--LARGE DATA--/#Pizzi/TBI-sham rbc velocity/results/070825_pointscan_csvFiles/mht07_roiB/mht07_roiB_roi_signals.pkl\n",
      "‚úÖ Saved: /Users/jcoleman/Documents/--LARGE DATA--/#Pizzi/TBI-sham rbc velocity/results/070825_pointscan_csvFiles/mht08_roiA/mht08_roiA_roi_signals.pkl\n",
      "   ***   \n",
      "üìù Metadata TXT saved (eg: /Users/jcoleman/Documents/--LARGE DATA--/#Pizzi/TBI-sham rbc velocity/results/070825_pointscan_csvFiles/mht08_roiA)\n",
      "üìÑ Logged processed and skipped files in subdirectories of /Users/jcoleman/Documents/--LARGE DATA--/#Pizzi/TBI-sham rbc velocity/results/070825_pointscan_csvFiles\n"
     ]
    }
   ],
   "source": [
    "# Alternative for batch via HiperGator etc\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# Step 1: Load directory list from text file\n",
    "with open(\"dir_list.txt\", \"r\") as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "master_dir = lines[0]\n",
    "subdirs = lines[1:]\n",
    "\n",
    "# Step 2: Loop through each subdirectory\n",
    "for subdir in subdirs:\n",
    "    input_dir = os.path.join(master_dir, subdir)\n",
    "    if not os.path.isdir(input_dir):\n",
    "        print(f\"‚ö†Ô∏è Skipping missing directory: {input_dir}\")\n",
    "        continue\n",
    "\n",
    "    # Step 3: Load all matching CSV files\n",
    "    # csv_files = [f for f in os.listdir(input_dir) if f.endswith(\".csv\") and \"output_roi\" in f]\n",
    "    csv_files = [\n",
    "        f for f in os.listdir(input_dir)\n",
    "        if (\n",
    "            f.endswith(\".csv\")\n",
    "            and f.startswith(\"output_roi\")\n",
    "            and \"frame\" in f\n",
    "            and \"Z\" in f\n",
    "            and \"_valley_\" not in f\n",
    "        )\n",
    "    ]\n",
    "    if not csv_files:\n",
    "        print(f\"‚ö†Ô∏è No matching CSVs found in: {input_dir}\")\n",
    "        continue\n",
    "\n",
    "    csv_data = {\n",
    "        filename: pd.read_csv(os.path.join(input_dir, filename), header=None)\n",
    "        for filename in csv_files\n",
    "    }\n",
    "\n",
    "    # Step 4: Save the dictionary to a pickle file named after the subdirectory\n",
    "    pickle_filename = f\"{subdir}_roi_signals.pkl\"\n",
    "    pickle_path = os.path.join(input_dir, pickle_filename)\n",
    "    with open(pickle_path, \"wb\") as f:\n",
    "        pickle.dump(csv_data, f)\n",
    "\n",
    "    print(f\"‚úÖ Saved: {pickle_path}\")\n",
    "\n",
    "    # Step 4: Create metadata with file info and Z\n",
    "    meta = []\n",
    "    for fname, df in csv_data.items():\n",
    "        # Remove extension first, then extract Z value\n",
    "        fname_no_ext = os.path.splitext(fname)[0]  # removes '.csv'\n",
    "        z_match = re.search(r'Z-?([\\d.]+)', fname_no_ext)\n",
    "        z_pos = float(z_match.group(1)) if z_match else None\n",
    "    \n",
    "        meta.append({\n",
    "            \"filename\": fname,\n",
    "            \"rows\": len(df),\n",
    "            \"min\": df.min().iloc[0],\n",
    "            \"max\": df.max().iloc[0],\n",
    "            \"std\": df.std().iloc[0],\n",
    "            \"n_unique\": df.nunique().iloc[0],\n",
    "            \"z_motor_microns\": z_pos\n",
    "        })\n",
    "\n",
    "        # Convert metadata to DataFrame\n",
    "        meta_df = pd.DataFrame(meta)\n",
    "\n",
    "        # Step 5: Get all CSVs in the folder (just for logging purposes)\n",
    "        all_csvs = [f for f in os.listdir(input_dir) if f.endswith(\".csv\")]\n",
    "        \n",
    "        # Determine skipped files\n",
    "        skipped_files = sorted(set(all_csvs) - set(csv_files))\n",
    "        processed_files = sorted(csv_files)\n",
    "\n",
    "            # Save to TXT (formatted string)\n",
    "    meta_txt_path = os.path.join(input_dir, f\"csv_metadata_{subdir}.txt\")\n",
    "    with open(meta_txt_path, \"w\") as f:\n",
    "        f.write(meta_df.to_string(index=False))\n",
    "    \n",
    "    #print(f\"üìù Metadata TXT saved: {meta_txt_path}\")\n",
    "        \n",
    "    # Write logs\n",
    "    # ADD DIRECTORY HEADER\n",
    "    with open(os.path.join(input_dir, f\"processed_files_{subdir}.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(processed_files))\n",
    "    \n",
    "    with open(os.path.join(input_dir, f\"skipped_files_{subdir}.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(skipped_files))\n",
    "    \n",
    "    #print(f\"üìÑ Logged {len(processed_files)} processed and {len(skipped_files)} skipped files in {input_dir}\")\n",
    "\n",
    "outdir = os.path.split(input_dir)\n",
    "metaoutdir = os.path.split(meta_txt_path)\n",
    "print(\"   ***   \")\n",
    "print(f\"üìù Metadata TXT saved to respective subdirectories (eg: {metaoutdir[0]})\")\n",
    "print(f\"üìÑ Logged processed and skipped files in subdirectories of {outdir[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d39e3c6b-d863-41cf-9dee-173616ecfae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PICKLE FILE\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tkinter import Tk, filedialog\n",
    "\n",
    "# Open dialog to select a folder\n",
    "root = Tk()\n",
    "root.withdraw()  # Hide the root window\n",
    "pickle_path = filedialog.askopenfilename(title=\"Select a PKL file\") \n",
    "\n",
    "# # SAVE\n",
    "# # Save pickle in the same directory\n",
    "# # pickle_path = os.path.join(input_dir, \"roi_signals.pkl\")\n",
    "# with open(pickle_path, \"wb\") as f:\n",
    "#     pickle.dump(csv_data, f)\n",
    "# print(f\"Saved pickle to: {pickle_path}\")\n",
    "\n",
    "# LOAD Pickle file\n",
    "# pickle_path = os.path.join(input_dir, \"roi_signals.pkl\")\n",
    "with open(pickle_path, \"rb\") as f:\n",
    "    csv_data_tmp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41514aca-2288-4895-9c1a-77d59be4bbbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>559790.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>553180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>583450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>572700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>587820.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3602</th>\n",
       "      <td>198110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3603</th>\n",
       "      <td>219010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604</th>\n",
       "      <td>187640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3605</th>\n",
       "      <td>201780.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3606</th>\n",
       "      <td>204920.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3607 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "0     559790.0\n",
       "1     553180.0\n",
       "2     583450.0\n",
       "3     572700.0\n",
       "4     587820.0\n",
       "...        ...\n",
       "3602  198110.0\n",
       "3603  219010.0\n",
       "3604  187640.0\n",
       "3605  201780.0\n",
       "3606  204920.0\n",
       "\n",
       "[3607 rows x 1 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data.keys()\n",
    "csv_data['output_roi4_frame3_Z85.4.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3e29baef-6828-46fc-b9a1-bf37e05b02e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_data.items()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a96ab9f-befe-41fe-9787-6587ac92d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers for extracted data (valley_durations are the estimated velocity measures for each valley/RBC)\n",
    "headers_main = ['filename', 'rows',  'min', 'max', 'std' , 'n_unique', 'z_motor_microns']\n",
    "\n",
    "headers_data = ['filename', 'roiNum', 'frameNum', 'valley_counts',\n",
    "           'flux', 'mean_velocity', 'std_velocity', 'valley_durations',\n",
    "           'datapoint_rows', 'datapoint_seconds', 'threshold',\n",
    "           'prominence', 'SNR', 'min', 'max', 'z_motor_microns',\n",
    "           'peak_counting_algorithmID']\n",
    "#                      filename  rows  min      max           std  n_unique  z_motor_microns\n",
    "# output_roi2_frame1_Z-38.7.csv  3607  0.0  68115.0  12052.769498      3461             38.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be30cb89-9baa-4973-93fa-b1f0118d4e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['filename',\n",
       " 'roiNum',\n",
       " 'frameNum',\n",
       " 'valley_counts',\n",
       " 'flux',\n",
       " 'mean_velocity',\n",
       " 'std_velocity',\n",
       " 'valley_durations',\n",
       " 'datapoint_rows',\n",
       " 'datapoint_seconds',\n",
       " 'threshold',\n",
       " 'prominence',\n",
       " 'SNR',\n",
       " 'min',\n",
       " 'max',\n",
       " 'z_motor_microns',\n",
       " 'peak_counting_algorithmID']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe97d826-d0f6-48d1-bad6-988ea4d98416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Miniforge3 (data_analysis_env)",
   "language": "python",
   "name": "data_analysis_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
